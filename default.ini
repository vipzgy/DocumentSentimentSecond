[Data]
data_dir = data
para_train_file = %(data_dir)s/3/train_paragraph6.txt
sen_train_file = %(data_dir)s/3/train_sentence6.txt
para_dev_file =
sen_dev_file =
para_test_file = %(data_dir)s/3/test_paragraph6.txt
sen_test_file = %(data_dir)s/3/test_sentence6.txt
embedding_file = %(data_dir)s/wiki.zh.vec
vocab_size = 20000
max_length = 200
shuffle = true

[Save]
save_dir = snapshot
config_file = %(save_dir)s/models/default.ini
save_model_path = %(save_dir)s/models
save_feature_voc = %(save_dir)s/feature2id.pkl
save_label_voc = %(save_dir)s/label2id.pkl

load_dir = snapshot
load_model_path = %(load_dir)s/models
load_feature_voc = %(load_dir)s/feature2id.pkl
load_label_voc = %(load_dir)s/label2id.pkl

train_pkl = %(save_dir)s/train.pkl
dev_pkl = %(save_dir)s/dev.pkl
test_pkl = %(save_dir)s/test.pkl
embedding_pkl = %(save_dir)s/embedding.pkl

[Network]
embedding_dim = 300
dropout_embed = 0.5

s_num_layers = 1
s_hidden_size = 200
s_dropout_rnn = 0.5
p_num_layers = 1
p_hidden_size = 100
p_dropout_rnn = 0.5

s_attention_size = 150
p_attention_size = 50

in_channels = 1
out_channels = 200
kernel_sizes = 3,4,5

max_norm = 5.0
which_model = HierarchicalAttentionLSTM

[Optimizer]
learning_algorithm = adam
lr = 0.001
lr_scheduler =
weight_decay = 1e-6
clip_norm = 10

[Run]
epochs = 300
batch_size = 150
test_interval = 100
save_after = 3
